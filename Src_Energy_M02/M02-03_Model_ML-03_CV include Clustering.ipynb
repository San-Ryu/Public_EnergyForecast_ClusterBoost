{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Hist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - CODE : Model - KIER Method 02(Clustering)  \n",
    " - DESC : 각 군집별 Model Analysis 및 Evaluation  \n",
    " - DATE  \n",
    "   &ensp; 2024-08-20 Created : \"M02-03_Model_ML-01_Single.ipynb\"에 Clustering 및 각 군집화 Case별 Cross Validation 적용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-01. Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-01-01. Init_Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region Basic_Import\n",
    "## Basic\n",
    "import os, sys, warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.path.dirname(os.path.abspath('./__file__'))\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('./__file__'))))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "pd.options.display.float_format = '{:.10f}'.format\n",
    "\n",
    "import math, random\n",
    "\n",
    "## Datetime\n",
    "import time, datetime as dt\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "## glob\n",
    "import glob, requests, json\n",
    "from glob import glob\n",
    "\n",
    "## 시각화\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "# %matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "## Split, 정규화\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# K-Means 알고리즘\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "# Clustering 알고리즘의 성능 평가 측도\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, silhouette_score, rand_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "## Modeling, Model Training\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "## Grid Search\n",
    "# kfold = KFold(n_splits = 5, shuffle = False, random_state = None)\n",
    "\n",
    "## For Web\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlencode, unquote, quote_plus\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "#endregion Basic_Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Imported\n"
     ]
    }
   ],
   "source": [
    "## Import_DL\n",
    "str_tar = \"tf\"\n",
    "## For Torch\n",
    "if str_tar == \"torch\":\n",
    "    import torch, torch.nn as nn\n",
    "    from torch.nn.utils import weight_norm\n",
    "    print(\"Torch Imported\")\n",
    "## For TF\n",
    "elif str_tar == \"tf\":\n",
    "    import tensorflow as tf, tensorflow_addons as tfa\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.models import Sequential, load_model\n",
    "    from keras_flops import get_flops\n",
    "    print(\"Tensorflow Imported\")\n",
    "else:\n",
    "    print(\"Error : Cannot be used except for Keywords\")\n",
    "    print(\" : torch / tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import_Local\n",
    "from Src_Dev_Common import Data_Datetime as com_date, KMA_Weather as com_KMA, KECO_AirKor as com_KECO, KASI_Holiday as com_Holi, KIER_Usage_M02 as com_KIER_M02, Data_Analysis as com_Analysis, Common_Model_ML as com_ML, Data_Clustering as com_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-01-02. Config (Directory, Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init_config\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"], os.environ['TF_DETERMINISTIC_OPS'] = str(SEED), \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-13 10:32:21.276296\n",
      "2024 / 9 / 13\n",
      "10 : 32\n"
     ]
    }
   ],
   "source": [
    "## Define Todate str\n",
    "str_now_ymd = pd.datetime.now().date()\n",
    "str_now_y, str_now_m, str_now_d = pd.datetime.now().year, pd.datetime.now().month, pd.datetime.now().day\n",
    "str_now_hr, str_now_min = pd.datetime.now().hour, pd.datetime.now().minute\n",
    "\n",
    "print(pd.datetime.now())\n",
    "print(str(str_now_y) + \" / \" + str(str_now_m)  + \" / \" + str(str_now_d))\n",
    "print(str(str_now_hr) + \" : \" + str(str_now_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-01. 군집화 부분 함수화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-01-01. KIER (Energy Usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_label():\n",
    "    df_kier_raw = pd.read_csv(str_dirName_h + str_file, index_col = 0)\n",
    "    df_kier_raw['METER_DATE'] = pd.to_datetime(df_kier_raw['METER_DATE'])\n",
    "\n",
    "    ## 호실별 순시 사용량 컬럼만 가져오기\n",
    "    list_col_tar = list(df_kier_raw.columns[1:])\n",
    "    df_kier_h = df_kier_raw.set_index('METER_DATE')\n",
    "\n",
    "    # ## Error Log : \"[5:-2]\" 부분을 추가하여 연월일시 및 평균합계 부분을 제거해주지 않으면, 군집화 계수가 제대로 도출되지 못함.\n",
    "    # df_kier_summary_total = df_kier_h.transpose().reset_index()[5:-2]\n",
    "    # ## 또는, 가장 깔끔하게 이렇게 처리해도 좋다\n",
    "    df_kier_summary_total = df_kier_h[list_col_tar].transpose().reset_index()\n",
    "\n",
    "    ## 세대 번호의 컬럼명이 'index'로 지정되어 오류 발생\n",
    "    df_kier_summary_total['h_index'] = df_kier_summary_total['index']\n",
    "    df_kier_summary_total = df_kier_summary_total.drop(columns = ['index'])\n",
    "\n",
    "    X = df_kier_summary_total.drop(columns = 'h_index')\n",
    "    y = df_kier_summary_total['h_index']\n",
    "\n",
    "    # 변수 표준화\n",
    "    scaler = StandardScaler() # 변수 표준화 클래스\n",
    "    scaler.fit(X)  # 표준화를 위해 변수별 파라미터(평균, 표준편차) 계산\n",
    "    X_std = scaler.transform(X)  # 훈련자료 표준화 변환\n",
    "\n",
    "    ## 최종 군집에 대한 Labeled Data 저장\n",
    "    km = KMeans(n_clusters = K, init=\"k-means++\", max_iter=300, n_init=1).fit(X_std)\n",
    "    list_size_cluster = com_clustering.get_cluster_sizes(km, X_std) ## 최종 군집화에 대한 군집 크기\n",
    "    df_kier_summary_total['target_'+str_domain] = 0\n",
    "    for i in range(0, len(df_kier_summary_total)) : df_kier_summary_total['target_'+str_domain].iloc[i] = km.labels_[i]\n",
    "\n",
    "    str_file_labeled = str_dirName_h + 'KIER_' + str(str_domain) + '_Labeled_' + str_interval + '_K' + str(K) + '.csv'\n",
    "    df_kier_summary_total = df_kier_summary_total[['h_index', 'target_'+str_domain]]\n",
    "    df_kier_summary_total.to_csv(str_file_labeled)\n",
    "\n",
    "    return df_kier_summary_total, list_size_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-02. Data Load 및 준비 부분 함수화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-02-01. KMA ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_Not_cluster():\n",
    "    ## ▶ Dataset 불러오기\n",
    "    ## 1. Interpolate / Filled ASOS Data\n",
    "    str_file = '../data_Energy_KIER/KMA_ASOS_119_2010_2023_1st_to CSV.csv'\n",
    "    df_ASOS = pd.read_csv(str_file, index_col = 0).reset_index()\n",
    "\n",
    "    try : df_ASOS['METER_DATE'] = pd.to_datetime(df_ASOS['METER_DATE'])\n",
    "    except KeyError : df_kier_raw = com_date.create_col_datetime(df_ASOS, 'METER_DATE', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE').drop(labels = ['None'], axis = 1)\n",
    "\n",
    "    ## 3. 1시간 단위 사용량 Data Load\n",
    "    str_file = 'KIER_' + str_domain + '_INST_1H_Resampled.csv'\n",
    "    df_raw = pd.read_csv(str_dirName_h + str_file, index_col = 0)\n",
    "\n",
    "    ## ▶ h_index에 따라 Dataset 분리\n",
    "    ## 1. 각 index별 house 목록 생성\n",
    "    list_kier_h_all = df_kier_h_cluster['h_index']\n",
    "\n",
    "    ## 2. 전체 사용량 합계 구하기\n",
    "    df_kier_h_all = df_raw.copy()\n",
    "    df_kier_h_all['METER_DATE'] = pd.to_datetime(df_kier_h_all['METER_DATE'])\n",
    "    df_kier_h_tmp = df_raw[list_kier_h_all]\n",
    "    df_kier_h_all[str_domain + '_INST_SUM_ALL'] = df_kier_h_tmp.sum(axis = 1)\n",
    "    ## 시점을 밀어서, 세대별 사용량을 과거 사용량으로 사용\n",
    "    df_kier_h_all[str_domain + '_INST_SUM_ALL'] = df_kier_h_all[str_domain + '_INST_SUM_ALL'].shift(1)\n",
    "    df_kier_h_all.dropna()\n",
    "\n",
    "    ## 4. 날씨 데이터 추가\n",
    "    df_kier_h_all = pd.merge(df_kier_h_all, df_ASOS, how = 'left', on = ['METER_DATE'])\n",
    "    df_kier_h_all = com_KMA.Interpolate_KMA_ASOS(df_kier_h_all)\n",
    "    df_kier_h_all = com_date.create_col_ymdhm(df_kier_h_all, 'METER_DATE')\n",
    "\n",
    "    # str_col_tar = str_domain + '_INST_SUM_' + dict_grp[int_grp]\n",
    "    str_col_tar = str_domain + '_INST_SUM_ALL'\n",
    "    df_tar_res = df_kier_h_all.drop(columns = ['METER_DATE', 'DAY']).dropna()\n",
    "\n",
    "    return df_tar_res, str_col_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_cluster(int_grp):\n",
    "    ## ▶ Dataset 불러오기\n",
    "    ## 1. Interpolate / Filled ASOS Data\n",
    "    str_file = '../data_Energy_KIER/KMA_ASOS_119_2010_2023_1st_to CSV.csv'\n",
    "    df_ASOS = pd.read_csv(str_file, index_col = 0).reset_index()\n",
    "\n",
    "    try : df_ASOS['METER_DATE'] = pd.to_datetime(df_ASOS['METER_DATE'])\n",
    "    except KeyError : df_kier_raw = com_date.create_col_datetime(df_ASOS, 'METER_DATE', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE').drop(labels = ['None'], axis = 1)\n",
    "\n",
    "    ## 2. Labeled Data Load\n",
    "    ## Cluster 기준 Interval\n",
    "    str_file_clustering = 'KIER_' + str(str_domain) + '_Labeled_' + str_interval + '_K' + str(K) + '.csv'\n",
    "    df_kier_h_cluster = pd.read_csv(str_dirName_h + str_file_clustering\n",
    "                                    , index_col = 0).rename(columns = {'index' : 'h_index'})[['h_index', 'target_' + str_domain]]\n",
    "    # print(str_interval)\n",
    "    # print(df_kier_h_cluster['target_' + str_domain].drop_duplicates())\n",
    "    # df_kier_h_cluster\n",
    "\n",
    "    ## 3. 1시간 단위 사용량 Data Load\n",
    "    str_file = 'KIER_' + str_domain + '_INST_1H_Resampled.csv'\n",
    "    df_raw = pd.read_csv(str_dirName_h + str_file, index_col = 0)\n",
    "\n",
    "\n",
    "\n",
    "    ## ▶ h_index에 따라 Dataset 분리\n",
    "    ## 1. 각 index별 house 목록 생성\n",
    "    list_kier_h_all = df_kier_h_cluster['h_index']\n",
    "    # print(len(list_kier_h_all))\n",
    "    list_kier_h_c0 = df_kier_h_cluster[df_kier_h_cluster['target_' + str_domain] == 0]['h_index']\n",
    "    # print(len(list_kier_h_c0))\n",
    "    list_kier_h_c1 = df_kier_h_cluster[df_kier_h_cluster['target_' + str_domain] == 1]['h_index']\n",
    "    # print(len(list_kier_h_c1))\n",
    "\n",
    "    if K == 3 : list_kier_h_c2 = df_kier_h_cluster[df_kier_h_cluster['target_' + str_domain] == 2]['h_index']\n",
    "    # print(len(list_kier_h_c2))\n",
    "\n",
    "    ## 2. 전체 사용량 합계 구하기\n",
    "    df_kier_h_all = df_raw.copy()\n",
    "    df_kier_h_all['METER_DATE'] = pd.to_datetime(df_kier_h_all['METER_DATE'])\n",
    "    df_kier_h_tmp = df_raw[list_kier_h_all]\n",
    "    df_kier_h_all[str_domain + '_INST_SUM_ALL'] = df_kier_h_tmp.sum(axis = 1)\n",
    "    ## 시점을 밀어서, 세대별 사용량을 과거 사용량으로 사용\n",
    "    df_kier_h_all[str_domain + '_INST_SUM_ALL'] = df_kier_h_all[str_domain + '_INST_SUM_ALL'].shift(1)\n",
    "    df_kier_h_all.dropna()\n",
    "\n",
    "    ## 3. Cluster별 사용량 합계 산출\n",
    "    ## ■ C00\n",
    "    df_kier_h_c0 = df_raw.copy()[list_kier_h_c0]\n",
    "    df_kier_h_c0['METER_DATE'] = pd.to_datetime(df_kier_h_all['METER_DATE'])\n",
    "    df_kier_h_tmp = df_raw[list_kier_h_c0]\n",
    "    df_kier_h_c0[str_domain + '_INST_SUM_C0'] = df_kier_h_tmp.sum(axis = 1)\n",
    "    ## 시점을 밀어서, 세대별 사용량을 과거 사용량으로 사용\n",
    "    df_kier_h_c0[str_domain + '_INST_SUM_C0'] = df_kier_h_c0[str_domain + '_INST_SUM_C0'].shift(1)\n",
    "    df_kier_h_c0.dropna()\n",
    "\n",
    "    ## ■ C01\n",
    "    df_kier_h_c1 = df_raw.copy()[list_kier_h_c1]\n",
    "    df_kier_h_c1['METER_DATE'] = pd.to_datetime(df_kier_h_all['METER_DATE'])\n",
    "    df_kier_h_tmp = df_raw[list_kier_h_c1]\n",
    "    df_kier_h_c1[str_domain + '_INST_SUM_C1'] = df_kier_h_tmp.sum(axis = 1)\n",
    "    ## 시점을 밀어서, 세대별 사용량을 과거 사용량으로 사용\n",
    "    df_kier_h_c1[str_domain + '_INST_SUM_C1'] = df_kier_h_c1[str_domain + '_INST_SUM_C1'].shift(1)\n",
    "    df_kier_h_c1.dropna()\n",
    "\n",
    "    if K == 3:\n",
    "        ## ■ C02\n",
    "        df_kier_h_c2 = df_raw.copy()[list_kier_h_c2]\n",
    "        df_kier_h_c2['METER_DATE'] = pd.to_datetime(df_kier_h_all['METER_DATE'])\n",
    "        df_kier_h_tmp = df_raw[list_kier_h_c2]\n",
    "        df_kier_h_c2[str_domain + '_INST_SUM_C2'] = df_kier_h_tmp.sum(axis = 1)\n",
    "        ## 시점을 밀어서, 세대별 사용량을 과거 사용량으로 사용\n",
    "        df_kier_h_c2[str_domain + '_INST_SUM_C2'] = df_kier_h_c2[str_domain + '_INST_SUM_C2'].shift(1)\n",
    "        df_kier_h_c2.dropna()\n",
    "\n",
    "    ## 4. 날씨 데이터 추가\n",
    "    df_kier_h_all = pd.merge(df_kier_h_all, df_ASOS, how = 'left', on = ['METER_DATE'])\n",
    "    df_kier_h_all = com_KMA.Interpolate_KMA_ASOS(df_kier_h_all)\n",
    "    df_kier_h_all = com_date.create_col_ymdhm(df_kier_h_all, 'METER_DATE')\n",
    "\n",
    "    df_kier_h_c0 = pd.merge(df_kier_h_c0, df_ASOS, how = 'left', on = ['METER_DATE'])\n",
    "    df_kier_h_c0 = com_KMA.Interpolate_KMA_ASOS(df_kier_h_c0)\n",
    "    df_kier_h_c0 = com_date.create_col_ymdhm(df_kier_h_c0, 'METER_DATE')\n",
    "\n",
    "    df_kier_h_c1 = pd.merge(df_kier_h_c1, df_ASOS, how = 'left', on = ['METER_DATE'])\n",
    "    df_kier_h_c1 = com_KMA.Interpolate_KMA_ASOS(df_kier_h_c1)\n",
    "    df_kier_h_c1 = com_date.create_col_ymdhm(df_kier_h_c1, 'METER_DATE')\n",
    "\n",
    "    if K == 3:\n",
    "        df_kier_h_c2 = pd.merge(df_kier_h_c2, df_ASOS, how = 'left', on = ['METER_DATE'])\n",
    "        df_kier_h_c2 = com_KMA.Interpolate_KMA_ASOS(df_kier_h_c2)\n",
    "        df_kier_h_c2 = com_date.create_col_ymdhm(df_kier_h_c2, 'METER_DATE')\n",
    "\n",
    "    ## 모든 세대\n",
    "    if int_grp == 0 : df_tar_res = df_kier_h_all.drop(columns = ['METER_DATE', 'DAY']).dropna()\n",
    "    ## 군집 C0\n",
    "    elif int_grp == 1 : df_tar_res = df_kier_h_c0.drop(columns = ['METER_DATE', 'DAY']).dropna()\n",
    "    ## 군집 C1\n",
    "    elif int_grp == 2 : df_tar_res = df_kier_h_c1.drop(columns = ['METER_DATE', 'DAY']).dropna()\n",
    "    ## 군집 C0\n",
    "    elif int_grp == 3 : df_tar_res = df_kier_h_c2.drop(columns = ['METER_DATE', 'DAY']).dropna()\n",
    "\n",
    "    str_col_tar = str_domain + '_INST_SUM_' + dict_grp[int_grp]\n",
    "\n",
    "    return df_tar_res, str_col_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ELEC\n",
      "str_fileRaw : KIER_RAW_ELEC_2024-06-07.csv\n",
      "str_fileRaw_hList : KIER_RAW_ELEC_2024-06-07.csv\n",
      "str_file : KIER_ELEC_INST_1M_Resampled.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_index</th>\n",
       "      <th>target_ELEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ELEC_INST_EFF_561-1-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ELEC_INST_EFF_561-1-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ELEC_INST_EFF_561-1-3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ELEC_INST_EFF_561-1-4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ELEC_INST_EFF_561-2-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>ELEC_INST_EFF_563-23-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>ELEC_INST_EFF_563-23-3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>ELEC_INST_EFF_563-23-4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>ELEC_INST_EFF_563-24-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>ELEC_INST_EFF_563-24-2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    h_index  target_ELEC\n",
       "0     ELEC_INST_EFF_561-1-1            0\n",
       "1     ELEC_INST_EFF_561-1-2            0\n",
       "2     ELEC_INST_EFF_561-1-3            2\n",
       "3     ELEC_INST_EFF_561-1-4            0\n",
       "4     ELEC_INST_EFF_561-2-1            0\n",
       "..                      ...          ...\n",
       "343  ELEC_INST_EFF_563-23-2            0\n",
       "344  ELEC_INST_EFF_563-23-3            2\n",
       "345  ELEC_INST_EFF_563-23-4            1\n",
       "346  ELEC_INST_EFF_563-24-1            2\n",
       "347  ELEC_INST_EFF_563-24-2            2\n",
       "\n",
       "[348 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit \n",
    "np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "float_rate = 0.3\n",
    "# test_size = round(len(df_tar) * float_rate)\n",
    "int_fold = 10\n",
    "\n",
    "## Dict_Domain\n",
    "## {0:\"ELEC\", 1:\"HEAT\", 2:\"WATER\", 3:\"HOT_HEAT\", 4:\"HOT_FLOW\", 99:\"GAS\"}\n",
    "## K : 2 or 3\n",
    "## {0 : '10MIN', 1 : '1H', 2 : '1D', 3 : '1W', 4 : '1M'}\n",
    "## {0 : 'ALL', 1 : 'C0', 2 : 'C1', 3 : 'C2'}\n",
    "dict_model = {0 : 'CB', 1 : 'DT', 2 : 'LGBM', 3 : 'RF', 4 : 'XGB'}\n",
    "dict_interval = {0 : '10MIN', 1 : '1H', 2 : '1D', 3 : '1W', 4 : '1M'}\n",
    "dict_grp = {0 : 'ALL', 1 : 'C0', 2 : 'C1', 3 : 'C2'}\n",
    "int_domain, int_grp = 0, 1\n",
    "\n",
    "K = 3 ## 2, 3\n",
    "int_interval = 4 ## 3, 4\n",
    "int_model = 4 ## 0, 1, 2, 3, 4\n",
    "\n",
    "## Domain, ACCU/INST Column\n",
    "str_domain, str_col_accu, str_col_inst = com_KIER_M02.create_domain_str(int_domain)\n",
    "## Directory Root\n",
    "str_dirData, str_dir_raw, str_dir_cleansed, str_dirName_bld, str_dirName_h = com_KIER_M02.create_dir_str(str_domain)\n",
    "## Interval, Target File\n",
    "str_interval, str_fileRaw, str_fileRaw_hList, str_file = com_KIER_M02.create_file_str(str_domain, int_interval)\n",
    "\n",
    "# print(str(os.listdir(str_dirData)) + \"\\n\")\n",
    "# print(os.listdir(str_dirName_h))\n",
    "\n",
    "str_file_clustering = 'KIER_' + str(str_domain) + '_Labeled_' + str_interval + '_K' + str(K) + '.csv'\n",
    "df_kier_h_cluster = pd.read_csv(str_dirName_h + str_file_clustering\n",
    "                                , index_col = 0).rename(columns = {'index' : 'h_index'})[['h_index', 'target_' + str_domain]]\n",
    "df_kier_h_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 비군집화 데이터셋에 대한 별도 처리 (비교군)\n",
    "sys.stdout.flush() ## flush\n",
    "df_tar, str_col_tar = load_dataset_Not_cluster()\n",
    "list_res, list_hists = com_ML.model_ml_analysis_with_KFold(df_tar, int_model, float_rate, str_col_tar, int_fold, str_shuffle = True)\n",
    "\n",
    "## list_res 저장\n",
    "str_txt = '../kf_result_include_Clustering_' + dict_model[int_model] + '/kf_result_' + str(dict_interval[int_interval]) + '_ALL_' + dict_grp[int_grp] + '_' + dict_model[int_model] + '_CV' + str(int_fold) + '.txt'\n",
    "file_txt = open(str_txt, 'w')\n",
    "print('- Interval = ' + dict_interval[int_interval] + '\\n'\n",
    "        + '- K = 0' + '\\n'\n",
    "        + '- grp = ALL' + '\\n'\n",
    "        + '- model = ' + dict_model[int_model] + '\\n'\n",
    "        + '- Case = ALL' + ',' + ' size_cluster = ' + str(348) + '\\n'\n",
    "        + '- Size = ' + str(df_tar.shape) + '\\n'\n",
    "        + '- Columns = ' + str(df_tar.columns) + '\\n', file = file_txt)\n",
    "print(list_res, file = file_txt)\n",
    "\n",
    "## list_hist 저장\n",
    "str_txt = '../kf_hist_include_Clustering_' + dict_model[int_model] + '/kf_hist_' + str(dict_interval[int_interval]) + '_ALL_' + dict_grp[int_grp] + '_' + dict_model[int_model] + '_CV' + str(int_fold) + '.txt'\n",
    "file_txt = open(str_txt, 'w')\n",
    "print('- Interval = ' + dict_interval[int_interval] + '\\n'\n",
    "        + '- K = 0' + '\\n' \n",
    "        + '- grp = ALL' + '\\n'\n",
    "        + '- model = ' + dict_model[int_model] + '\\n'\n",
    "        + '- Case = ALL' + ',' + ' size_cluster = ' + str(348) + '\\n'\n",
    "        + '- Size = ' + str(df_tar.shape) + '\\n'\n",
    "        + '- Columns = ' + str(df_tar.columns) + '\\n', file = file_txt)\n",
    "print(list_hists, file = file_txt)\n",
    "\n",
    "## open 후 다른 것을 open하면 자동으로 close되어 저장되지만,\n",
    "## 마지막 파일은 반드시 close를 통해 종료해야만 저장이 완료됨\n",
    "file_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 결과창 용량이 너무 커져서 모델 1~2개씩 돌리는 것으로 수정\n",
    "# for int_model in range(4, 5) : ## 0:\"CB\", 1:\"DT\", 2:\"LGBM\", 3:\"RF\", 4:\"XGB\"\n",
    "#         ## 군집화 데이터셋에 대한 별도 처리\n",
    "#         for i in range (0, 10): ## 각 기간별 10번의 Clustering을 병행\n",
    "#                 sys.stdout.flush() ## flush\n",
    "#                 df_kier_summary_total, list_size_cluster = cluster_label()\n",
    "#                 print(list_size_cluster)\n",
    "#                 # df_kier_summary_total\n",
    "\n",
    "#                 for int_grp in range(1, K + 1): ## 군집 형성된 데이터셋만 분석\n",
    "#                         print('■ ' + str(int_grp))\n",
    "#                         df_tar, str_col_tar = load_dataset_cluster(int_grp) ## 해당 군집에 대한 데이터셋 및 Target Column\n",
    "#                         # print(df_tar.columns)\n",
    "#                         # print(df_tar.shape)\n",
    "\n",
    "#                         list_res, list_hists = com_ML.model_ml_analysis_with_KFold(df_tar, int_model, float_rate, str_col_tar, int_fold, str_shuffle = True)\n",
    "\n",
    "#                         ## list_res 저장\n",
    "#                         str_txt = '../kf_result_include_Clustering_' + dict_model[int_model] + '/kf_result_' + str(dict_interval[int_interval]) + '_K'  + str(K)  + '_Case0' + str(i) + '_' + dict_grp[int_grp] + '_' + dict_model[int_model] + '_CV' + str(int_fold) + '.txt'\n",
    "#                         file_txt = open(str_txt, 'w')\n",
    "#                         print('- Interval = ' + dict_interval[int_interval] + '\\n'\n",
    "#                                 + '- K = ' + str(K) + '\\n'\n",
    "#                                 + '- grp = C0' + str(int_grp) + '\\n'\n",
    "#                                 + '- model = ' + dict_model[int_model] + '\\n'\n",
    "#                                 + '- Case = 0' + str(i) + ',' + ' size_cluster = ' + str(list_size_cluster) + '\\n'\n",
    "#                                 + '- Size = ' + str(df_tar.shape) + '\\n'\n",
    "#                                 + '- Columns = ' + str(df_tar.columns) + '\\n', file = file_txt)\n",
    "#                         print(list_res, file = file_txt)\n",
    "\n",
    "#                         ## list_hist 저장\n",
    "#                         str_txt = '../kf_hist_include_Clustering_' + dict_model[int_model] + '/kf_hist_' + str(dict_interval[int_interval]) + '_K'  + str(K)  + '_Case0' + str(i) + '_' + dict_grp[int_grp] + '_' + dict_model[int_model] + '_CV' + str(int_fold) + '.txt'\n",
    "#                         file_txt = open(str_txt, 'w')\n",
    "#                         print('- Interval = ' + dict_interval[int_interval] + '\\n'\n",
    "#                                 + '- K = ' + str(K) + '\\n'\n",
    "#                                 + '- grp = C0' + str(int_grp) + '\\n'\n",
    "#                                 + '- model = ' + dict_model[int_model] + '\\n'\n",
    "#                                 + '- Case = 0' + str(i) + ',' + ' size_cluster = ' + str(list_size_cluster) + '\\n'\n",
    "#                                 + '- Size = ' + str(df_tar.shape) + '\\n'\n",
    "#                                 + '- Columns = ' + str(df_tar.columns) + '\\n', file = file_txt)\n",
    "#                         print(list_hists, file = file_txt)\n",
    "\n",
    "#                         ## open 후 다른 것을 open하면 자동으로 close되어 저장되지만,\n",
    "#                         ## 마지막 파일은 반드시 close를 통해 종료해야만 저장이 완료됨\n",
    "#                         file_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dev-ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
